# 大数据挖掘
## 天气自动传感器大数据
### 数据处理思路


先分析数据分布情况，然后进行数据清洗，最后进行分类
### 数据分布
采用pandas库进行数据分析
使用pandas库的describe()方法进行数据描述。
```python
import pandas as pd
df = pd.read_csv('data.csv')
df.describe()
```
得到数据量，均值，中位数，最大值，最小值，四分位数，标准差等信息。
![Alt text](image-4.png)
Wave_Height标准差为12220.244834544077很大，需要特殊处理
Wave_Period的标准差为12220.69686378749也很大，需要特殊处理
### 数据处理
不是对一列数据进行处理，而是对属于每个海滩的各个特征进行处理

因为每个沙滩的情况不一样，所以需要对每个沙滩进行处理
```python
Wave_Height = pd.DataFrame.from_dict(Wave_Height_label_dict,orient='index')
Wave_Height.to_csv('Wave_Height.csv')
Wave_Height.apply(pd.Series.describe, axis=1).to_csv('Wave_Height_Features.csv')

Wave_Period = pd.DataFrame.from_dict(Wave_Period_label_dict,orient='index')
Wave_Period.to_csv('Wave_Period.csv')
Wave_Period.apply(pd.Series.describe, axis=1).to_csv('Wave_Period_Features.csv')
```
Wave_Height:
![Alt text](image-6.png)
Wave_Period:
![Alt text](image-7.png)
可以发现Ohio Street Beach、63rd Street Beach、Rainbow Beach三个沙滩的海滩高度和周期有异常值

理论：如果数据点比第一个四分位数低 1.5 乘 IQR，或比第三个四分位数高 1.5 乘 IQR，就属于离群或极度离群
```python
Wave_Height = pd.DataFrame.from_dict(Wave_Height_label_dict,orient='index')
Wave_Height = Wave_Height.transpose()
Wave_Height.to_csv('Wave_Height.csv')
Q1 = Wave_Height.quantile(0.25)
Q3 = Wave_Height.quantile(0.75)
IQR = Q3 - Q1
df_no_outliers_Wave_Height = \
Wave_Height[~((Wave_Height < \
(Q1 - 1.5 * IQR)) | (Wave_Height > (Q3 + 1.5 * IQR)))]
df_no_outliers_Wave_Height.to_csv('Wave_Height_no_outliers.csv')
df_no_outliers_Wave_Height.describe().\
to_csv('Wave_Height_no_outliers_features.csv')


Wave_Period = pd.DataFrame.from_dict(Wave_Period_label_dict,orient='index')
Wave_Period = Wave_Period.transpose()
Wave_Period.to_csv('Wave_Period.csv')
Q1 = Wave_Period.quantile(0.25)
Q3 = Wave_Period.quantile(0.75)
IQR = Q3 - Q1
df_no_outliers_Wave_Period = \
Wave_Period[~((Wave_Period < \
(Q1 - 1.5 * IQR)) | (Wave_Period > (Q3 + 1.5 * IQR)))]
df_no_outliers_Wave_Period.to_csv('Wave_Period_no_outliers.csv')
df_no_outliers_Wave_Period.describe().\
to_csv('Wave_Period_no_outliers_features.csv')
```
结果：
Wave_Height:
![Alt text](image-8.png)
Wave_Period:
![Alt text](image-9.png)

发现一些只是稍微比均值高正值也被剔除掉了，可能会影响训练效果
观察数据后采用仅仅剔除负值的形式对数据进行处理
```python
Wave_Height = Wave_Height.transpose()
Wave_Period = Wave_Period.transpose()
Wave_Height = Wave_Height[Wave_Height >= 0].dropna()
Wave_Period = Wave_Period[Wave_Period >= 0].dropna()
Wave_Height.to_csv('Wave_Height_no_negative.csv')
Wave_Height.describe().\
to_csv('Wave_Height_no_negative_features.csv')
Wave_Period.to_csv('Wave_Period_no_negative.csv')
Wave_Period.describe().\
to_csv('Wave_Period_no_negative_features.csv')
```

结果：
![Alt text](image-10.png)
![Alt text](image-11.png)
这样效果也不错,但是这样后数据只有2627行了

看来只有每个沙滩值和特征值都分开来处理了


```python
def Dict_to_list(self,data_dict):#用均值填充NaN
X = []
Y = []
X_average ={}
X_median = {}
for key,values in data_dict.items():
    index_num=[]
    X_Temp=[]
    for index, element in enumerate(values):
        if math.isnan(element):
            index_num.append(index)
        else:
            X_Temp.append(element)
    if X_Temp != []:
        X_average[key]=np.mean(X_Temp)
        X_median[key]=np.median(X_Temp)
    else:
        X_average[key]=0
        X_median[key]=0
    if index_num != []:#有NaN,马上替换
        for index in index_num:
            values[index]=X_average[key] 

    X.append(values)
    Y.append([key]*len(values))

# X = np.array(X)
#Y = np.array(Y)
print("Yes,Dict_to_list complete!")
return X,Y

```

之后发现了更简洁的做法(哭)
```python
# 读取CSV文件
df = pd.read_csv('data.csv')
# 指定需要计算均值和填充的列
columns_to_process = ['Water_Temperature', 'Turbidity', 'Wave_Height','Wave_Period']
# 计算均值
mean_values = df[columns_to_process].\
apply(lambda x: np.mean(x[x >= 0]), axis=0)
# 填充负值和NaN值
df[columns_to_process] = df[columns_to_process].\
apply(lambda x: x.mask((x < 0) | x.isna(), mean_values[x.name]), axis=0)
# 将结果保存回原CSV文件
df.to_csv('Data.csv', index=False)
```



















最终接口：
```python
X = np.array([[5.1, 3.5, 1.4, 0.2],
              [4.9, 3.0, 1.4, 0.2],
              [4.7, 3.2, 1.3, 0.2],
              [7.0, 3.2, 4.7, 1.4],
              [6.4, 3.2, 4.5, 1.5]])


y = np.array([0, 0, 0, 1, 1])
```



构造X,y接口：



先把数据格式处理为：{'Montrose Beach':[1,2,3,4],'Ohio Street Beach':[1,2,3,4],'Calumet Beach':[1,2,3,4],'63rd Street Beach':[1,2,3,4]}
然后
处理为
X=[[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]]
y=[['Montrose Beach'],['Ohio Street Beach'],['Calumet Beach'],['63rd Street Beach']]
```python
data = {'Montrose Beach':[1,2,3,4],'Ohio Street Beach':[1,2,3,4],'Calumet Beach':[1,2,3,4],'63rd Street Beach':[1,2,3,4]}
X = []
y = []
for key, value in data.items():
    X.append(value)
    y.append([key])
```
```python
def Features_Get(data):
    Features_label_dict = {}

    i=0
    for index,row in data.iterrows():#遍历每一行
        Beach_Name_key = row.iloc[0]
        Features_values_Water_Temperature = row.iloc[2]
        Features_values_Turbidity = row.iloc[3]
        Features_values_Wave_Height = row.iloc[5]
        Features_values_Wave_Period = row.iloc[6]


        if Beach_Name_key in Features_label_dict:
            Features_label_dict[Beach_Name_key].append(Features_values_Water_Temperature)
            Features_label_dict[Beach_Name_key].append(Features_values_Turbidity)
            Features_label_dict[Beach_Name_key].append(Features_values_Wave_Height)
            Features_label_dict[Beach_Name_key].append(Features_values_Wave_Period)
        else:
            Features_label_dict[Beach_Name_key]=[Features_values_Water_Temperature,Features_values_Turbidity,Features_values_Wave_Height,Features_values_Wave_Period]
    
        i+=1
        if i==3:
            break
        
    print(Features_label_dict)
    print("Yes,data cleaning complete!")
    return Features_label_dict
```

#### 清洗后



#### 分类方法说明
<font size=1>
1.数据类型：首先要了解数据的类型是离散型还是连续型。如果数据是离散型的，可以考虑使用决策树、朴素贝叶斯等方法。如果数据是连续型的，可以考虑使用支持向量机、逻辑回归等方法。

2.特征数量：特征数量是选择分类方法的重要考虑因素之一。如果特征数量较少，可以考虑使用简单的分类方法，如决策树、朴素贝叶斯等。如果特征数量较多，可以考虑使用随机森林、支持向量机等方法。

3.特征之间的相关性：特征之间的相关性也是选择分类方法的重要因素。如果特征之间存在较强的相关性，可以考虑使用线性分类方法，如逻辑回归、支持向量机等。如果特征之间的相关性较弱或不明显，可以考虑使用非线性分类方法，如决策树、随机森林等。

3.样本数量：样本数量也是选择分类方法的重要考虑因素之一。如果样本数量较少，可以考虑使用简单的分类方法，如朴素贝叶斯、支持向量机等。如果样本数量较多，可以考虑使用复杂的分类方法，如深度学习模型等。
#### 分类方法选择
样本数量在一到三万左右，
特征数量在1-6，
数据离散，
特征选取Water_Temperature、Turbidity、Wave_Height、Wave_Period，特征间关联不明显。

数据离散，排除支持向量机、逻辑回归
数据特征较少

选择随机森林或决策树
<font size=1>

决策树：

1. 样本数量：对于样本数量在一到三万左右的数据集，决策树是一个适用的分类算法。决策树算法在处理中等规模的数据集时具有良好的性能，并且可以快速构建模型。

2. 特征数量：特征数量为4个，这是一个相对较少的数量。决策树算法适用于特征数量较少的情况，因为它可以通过对每个特征的不同取值进行划分来构建决策树，从而进行分类。

3. 数据离散：决策树算法可以很好地处理离散数据。对于您的数据集，如果特征是离散的，决策树可以通过选择最佳的划分特征和取值来构建决策树，实现分类。

4. 特征选择：决策树算法可以通过特征选择来确定最具有分类能力的特征。在您的情况下，如果特征之间的关联不明显，决策树可以通过对每个特征的取值进行划分来选择最佳的特征，从而构建具有较高分类准确性的决策树模型。



随机森林：

1. 样本数量：随机森林算法在处理样本数量较大的数据集时表现良好。由于随机森林是通过集成多个决策树的预测结果来进行分类的，因此对于样本数量在一到三万左右的数据集，随机森林可以有效地提高分类准确性。

2. 特征数量：即使特征数量较少，随机森林仍然可以发挥优势。随机森林通过随机选择特征子集进行建立，减少了特征选择的复杂性。在您的情况下，特征数量为4个，适合使用随机森林算法进行分类。

3. 数据离散：随机森林算法对于离散数据具有良好的适应性。对于您的离散数据，随机森林可以通过选择最佳的划分特征和取值来构建决策树，从而实现分类。

4. 特征间关联不明显：即使特征之间的关联不明显，随机森林仍然可以发挥作用。由于随机森林是通过集成多个决策树的预测结果进行分类的，即使特征间的关联不明显，随机森林可以通过综合多个决策树的预测结果来提高分类准确性。




</font>




